{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "print(f'Using device: {device}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import time\n", "import numpy as np\n", "import math\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from torch.utils.data import DataLoader\n", "from torch import nn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from datetime import datetime"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Replace this to use Noisy QLSTM<br>\n", "from QLSTM_Noisy import SequenceDataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from QLSTMv1 import SequenceDataset  # <-- Your existing dataset class"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import yfinance as yf\n", "from pandas_datareader import data as pdr\n", "yf.pdr_override()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('infosys stock full combined.csv')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["columns = [\n", "    'Open Price', \n", "    'High Price', \n", "    'Low Price', \n", "    'Close Price',\n", "    'feature1','feature2','feature3','feature4','feature5'\n", "]\n", "# If you want to incorporate the multi-modal features, e.g., \"Encoded Text\" or \"Sentiment Score\",\n", "# you could add them to 'columns' or handle them separately. \n", "# columns += ['Sentiment Score']  # Example only if you want to treat them as numeric features."]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = df.filter(columns)\n", "dataset = data.values"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Splitting the data into train and test"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["size = int(len(df) * 0.7)\n", "df_train = dataset[:size].copy()\n", "df_test = dataset[size:].copy()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Select the features"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_train = pd.DataFrame(df_train, columns=columns)\n", "df_test = pd.DataFrame(df_test, columns=columns)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["features = df_train.columns\n", "target = 'Close Price'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def normalize(a, min_a=None, max_a=None):\n", "    if min_a is None:\n", "        min_a, max_a = np.min(a, axis=0), np.max(a, axis=0)\n", "    return (a - min_a) / (max_a - min_a + 0.0001), min_a, max_a"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Normalizing the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_train, min_train, max_train = normalize(df_train)\n", "df_test, _, _ = normalize(df_test, min_train, max_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["torch.manual_seed(101)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["batch_size = 1\n", "sequence_length = 3"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_dataset = SequenceDataset(\n", "    df_train,\n", "    target=target,\n", "    features=features,\n", "    sequence_length=sequence_length\n", ")\n", "test_dataset = SequenceDataset(\n", "    df_test,\n", "    target=target,\n", "    features=features,\n", "    sequence_length=sequence_length\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n", "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, y = next(iter(train_loader))\n", "print(\"Features shape:\", X.shape)\n", "print(\"Target shape:\", y.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_model(data_loader, model, loss_function, optimizer):\n", "    num_batches = len(data_loader)\n", "    total_loss = 0\n", "    model.train()\n", "    for X, y in data_loader:\n", "        X, y = X.to(device), y.to(device)\n", "        output = model(X)\n", "        loss = loss_function(output, y)\n", "        optimizer.zero_grad()\n", "        loss.backward()\n", "        optimizer.step()\n", "        total_loss += loss.item()\n", "    avg_loss = total_loss / num_batches\n", "    print(f\"Train loss: {avg_loss}\")\n", "    return avg_loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def test_model(data_loader, model, loss_function):\n", "    num_batches = len(data_loader)\n", "    total_loss = 0\n", "    model.eval()\n", "    with torch.no_grad():\n", "        for X, y in data_loader:\n", "            X, y = X.to(device), y.to(device)\n", "            output = model(X)\n", "            total_loss += loss_function(output, y).item()\n", "    avg_loss = total_loss / num_batches\n", "    print(f\"Test loss: {avg_loss}\")\n", "    return avg_loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predict(data_loader, model):\n", "    \"\"\"Just like `test_loop` function but keep track of the outputs instead of the loss\n", "    function.\n", "    \"\"\"\n", "    output = torch.tensor([]).to(device)\n", "    model.eval()\n", "    with torch.no_grad():\n", "        for X, _ in data_loader:\n", "            X = X.to(device)\n", "            y_star = model(X)\n", "            output = torch.cat((output, y_star), 0)\n", "    return output.cpu()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%% [ORIGINAL QShallowRegressionLSTM IMPORT & USAGE]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from QLSTMv1 import QShallowRegressionLSTM"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["learning_rate = 0.01\n", "num_hidden_units = 16"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Qmodel = QShallowRegressionLSTM(\n", "    num_sensors=len(features),\n", "    hidden_units=num_hidden_units,\n", "    n_qubits=7,\n", "    n_qlayers=1\n", ").to(device)\n", "loss_function = nn.MSELoss()\n", "optimizer = torch.optim.Adam(Qmodel.parameters(), lr=learning_rate)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Count number of parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_params = sum(p.numel() for p in Qmodel.parameters() if p.requires_grad)\n", "print(f\"Number of parameters (Original QLSTM): {num_params}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["quantum_loss_train = []\n", "quantum_loss_test = []\n", "num_epochs = 50"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for ix_epoch in range(num_epochs):\n", "    print(f\"Epoch {ix_epoch}\\n---------\")\n", "    start = time.time()\n", "    train_loss = train_model(train_loader, Qmodel, loss_function, optimizer=optimizer)\n", "    test_loss = test_model(test_loader, Qmodel, loss_function)\n", "    end = time.time()\n", "    print(\"Execution time\", end - start)\n", "    quantum_loss_train.append(train_loss)\n", "    quantum_loss_test.append(test_loss)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n", "test_eval_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ystar_col_Q = \"Model Forecast\"\n", "df_train[ystar_col_Q] = predict(train_eval_loader, Qmodel).cpu().numpy()\n", "df_test[ystar_col_Q] = predict(test_eval_loader, Qmodel).cpu().numpy()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12, 7))\n", "plt.plot(range(len(df_train)), df_train[\"Close Price\"], label = \"Real Data\")\n", "plt.plot(range(len(df_train)), df_train[\"Model Forecast\"], label = \"QLSTM Train Prediction\")\n", "plt.ylabel('Stock Price')\n", "plt.xlabel('Days')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12, 7))\n", "plt.plot(range(len(df_test)), df_test[\"Close Price\"], label = \"Real Data\")\n", "plt.plot(range(len(df_test)), df_test[\"Model Forecast\"], label = \"QLSTM Test Prediction\")\n", "plt.ylabel('Stock Price')\n", "plt.xlabel('Days')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Calculate the RMSE for the train and test data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import mean_squared_error"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_rmse = math.sqrt(mean_squared_error(df_train[\"Close Price\"], df_train[\"Model Forecast\"]))\n", "test_rmse = math.sqrt(mean_squared_error(df_test[\"Close Price\"], df_test[\"Model Forecast\"]))\n", "print(f\"Train RMSE: {train_rmse}\")\n", "print(f\"Test RMSE: {test_rmse}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Calculate the accuracy of the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def accuracy(y, y_star):\n", "    return np.mean(np.abs(y - y_star) < 0.1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_accuracy = accuracy(df_train[\"Close Price\"], df_train[\"Model Forecast\"])\n", "test_accuracy = accuracy(df_test[\"Close Price\"], df_test[\"Model Forecast\"])\n", "print(f\"Train accuracy: {train_accuracy}\")\n", "print(f\"Test accuracy: {test_accuracy}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Save the trained model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["torch.save(Qmodel.state_dict(), \"QLSTM_Stock_Price_Model.pt\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%% <br>\n", "##############################################################################<br>\n", "                       NEW ENHANCEMENTS / MODIFICATIONS                      #<br>\n", "##############################################################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Below we define a new model class: `EnhancedQLSTMModel`.<br>\n", "Key changes:<br>\n", "1) A classical LSTM layer is stacked in front of the QLSTM (hybrid approach).<br>\n", "2) We add a simple self-attention layer after the QLSTM outputs.<br>\n", "3) We include a skip connection around the QLSTM to help gradient flow.<br>\n", "4) Demonstrate how you could (optionally) handle multi-modal inputs <br>\n", "   (like 'Encoded Text' or 'Sentiment Score') by splitting them into <br>\n", "   a separate branch. This is just an example; adapt to your actual data usage.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from QLSTMv1 import QLSTM  # Import the QLSTM class directly if needed"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class EnhancedQLSTMModel(nn.Module):\n", "    def __init__(\n", "        self,\n", "        num_sensors,\n", "        hidden_units,\n", "        n_qubits=4,\n", "        n_qlayers=1,\n", "        multi_modal=False,\n", "        text_embedding_dim=4,\n", "        sentiment_dim=3\n", "    ):\n", "        super(EnhancedQLSTMModel, self).__init__()\n", "        \n", "        self.multi_modal = multi_modal\n", "        self.hidden_units = hidden_units\n", "        \n", "        # If multi_modal is True, we define separate heads for text and sentiment\n", "        if self.multi_modal:\n", "            # Example: you might have 'Encoded Text' as a 4D vector\n", "            self.text_branch = nn.Sequential(\n", "                nn.Linear(text_embedding_dim, 16),\n", "                nn.ReLU(),\n", "                nn.Linear(16, hidden_units),\n", "            )\n", "            # Example: you might have 'Sentiment Score' as a 3D vector\n", "            self.sentiment_branch = nn.Sequential(\n", "                nn.Linear(sentiment_dim, 16),\n", "                nn.ReLU(),\n", "                nn.Linear(16, hidden_units),\n", "            )\n", "        \n", "        # A classical LSTM to extract initial temporal features from the main numeric features\n", "        self.classical_lstm = nn.LSTM(\n", "            input_size=num_sensors,\n", "            hidden_size=hidden_units,\n", "            num_layers=1,\n", "            batch_first=True\n", "        )\n", "        \n", "        # QLSTM for the second stage\n", "        self.qlstm = QLSTM(\n", "            input_size=hidden_units,\n", "            hidden_size=hidden_units,\n", "            n_qubits=n_qubits,\n", "            n_qlayers=n_qlayers,\n", "            batch_first=True\n", "        )\n", "        \n", "        # A simple self-attention layer\n", "        self.attention = nn.MultiheadAttention(\n", "            embed_dim=hidden_units,\n", "            num_heads=2,\n", "            batch_first=True\n", "        )\n", "        \n", "        # Final linear layer for regression\n", "        self.output_layer = nn.Linear(hidden_units, 1)\n", "        \n", "    def forward(self, x, text_input=None, sentiment_input=None):\n", "        \"\"\"\n", "        Args:\n", "          x: shape [batch_size, seq_length, num_sensors]\n", "          text_input: optional, shape [batch_size, text_embedding_dim]\n", "          sentiment_input: optional, shape [batch_size, sentiment_dim]\n", "        \"\"\"\n", "        # If multi_modal, process the additional inputs\n", "        # and fuse them as needed.\n", "        if self.multi_modal and text_input is not None and sentiment_input is not None:\n", "            text_feat = self.text_branch(text_input)  # [batch_size, hidden_units]\n", "            sent_feat = self.sentiment_branch(sentiment_input)  # [batch_size, hidden_units]\n", "            # Expand dims to match time-series shape or incorporate them differently\n", "            # E.g., you could broadcast or simply add them to each timestep.\n", "            # For simplicity, let's just add them as a bias to x's first time step:\n", "            # x[:, 0, :hidden_units] += text_feat + sent_feat\n", "            # Alternatively, you might replicate them across all timesteps.\n", "        \n", "        # 1) Pass through classical LSTM\n", "        lstm_out, _ = self.classical_lstm(x)  # [batch_size, seq_length, hidden_units]\n", "        \n", "        # 2) Skip connection: we'll store the output of classical LSTM to add after QLSTM\n", "        skip_connection = lstm_out.clone()\n", "        \n", "        # 3) Pass through QLSTM\n", "        qlstm_out, _ = self.qlstm(lstm_out)   # [batch_size, seq_length, hidden_units]\n", "        \n", "        # 4) Simple skip connection: Add classical LSTM output to QLSTM output\n", "        combined_out = qlstm_out + skip_connection\n", "        \n", "        # 5) Apply attention. \n", "        #    Note: For multihead attention, the shape is (batch, seq, embed_dim).\n", "        attn_out, _ = self.attention(combined_out, combined_out, combined_out)\n", "        \n", "        # 6) We use the last time step for regression\n", "        last_step = attn_out[:, -1, :]  # shape [batch_size, hidden_units]\n", "        \n", "        # 7) Final regression output\n", "        out = self.output_layer(last_step).flatten()  # [batch_size]\n", "        return out"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%% "]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Below is an example usage of the new EnhancedQLSTMModel.<br>\n", "We will create an instance and train it similarly to your existing QShallowRegressionLSTM.<br>\n", "Comment out the original Qmodel training if you want to avoid double runs.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["enhanced_model = EnhancedQLSTMModel(\n", "    num_sensors=len(features),\n", "    hidden_units=num_hidden_units,\n", "    n_qubits=7,\n", "    n_qlayers=1,\n", "    multi_modal=False  # set to True if you plan to use 'Encoded Text' or 'Sentiment Score' \n", ").to(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loss_function_enh = nn.MSELoss()\n", "optimizer_enh = torch.optim.Adam(enhanced_model.parameters(), lr=learning_rate)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Count parameters in the enhanced model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_params_enh = sum(p.numel() for p in enhanced_model.parameters() if p.requires_grad)\n", "print(f\"Number of parameters (Enhanced QLSTM): {num_params_enh}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["enh_train_loss = []\n", "enh_test_loss = []\n", "num_epochs_enh = 50"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for ix_epoch in range(num_epochs_enh):\n", "    print(f\"[Enhanced] Epoch {ix_epoch}\\n---------\")\n", "    start = time.time()\n", "    t_loss = train_model(train_loader, enhanced_model, loss_function_enh, optimizer_enh)\n", "    v_loss = test_model(test_loader, enhanced_model, loss_function_enh)\n", "    end = time.time()\n", "    print(\"Execution time\", end - start)\n", "    enh_train_loss.append(t_loss)\n", "    enh_test_loss.append(v_loss)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate predictions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_train[\"Enhanced Forecast\"] = predict(train_eval_loader, enhanced_model).cpu().numpy()\n", "df_test[\"Enhanced Forecast\"] = predict(test_eval_loader, enhanced_model).cpu().numpy()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12, 7))\n", "plt.plot(range(len(df_train)), df_train[\"Close Price\"], label=\"Real Data\")\n", "plt.plot(range(len(df_train)), df_train[\"Enhanced Forecast\"], label=\"Enhanced Train Prediction\")\n", "plt.ylabel(\"Stock Price\")\n", "plt.xlabel(\"Days\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12, 7))\n", "plt.plot(range(len(df_test)), df_test[\"Close Price\"], label=\"Real Data\")\n", "plt.plot(range(len(df_test)), df_test[\"Enhanced Forecast\"], label=\"Enhanced Test Prediction\")\n", "plt.ylabel(\"Stock Price\")\n", "plt.xlabel(\"Days\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["enh_train_rmse = math.sqrt(mean_squared_error(df_train[\"Close Price\"], df_train[\"Enhanced Forecast\"]))\n", "enh_test_rmse = math.sqrt(mean_squared_error(df_test[\"Close Price\"], df_test[\"Enhanced Forecast\"]))\n", "print(f\"Enhanced Model Train RMSE: {enh_train_rmse}\")\n", "print(f\"Enhanced Model Test RMSE: {enh_test_rmse}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["enh_train_accuracy = accuracy(df_train[\"Close Price\"], df_train[\"Enhanced Forecast\"])\n", "enh_test_accuracy = accuracy(df_test[\"Close Price\"], df_test[\"Enhanced Forecast\"])\n", "print(f\"Enhanced Model Train accuracy: {enh_train_accuracy}\")\n", "print(f\"Enhanced Model Test accuracy: {enh_test_accuracy}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["torch.save(enhanced_model.state_dict(), \"Enhanced_QLSTM_Stock_Price_Model.pt\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}